apiVersion: batch/v1
kind: Job
metadata:
  name: btc-kd-combined
  namespace: csuf-titans
  labels:
    app: btc-training # Changed label
spec:
  backoffLimit: 2
  template:
    metadata:
      labels:
        app: btc-training # Changed label
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: nvidia.com/gpu.product
                operator: In
                values:
                - NVIDIA-A10
              - key: kubernetes.io/hostname
                operator: In
                values:
                - gpu-02.nrp.mghpcc.org
      restartPolicy: Never
      tolerations:
        - key: "node.kubernetes.io/not-ready"
          operator: "Exists"
          effect: "NoExecute"
          tolerationSeconds: 86400
        - key: "node.kubernetes.io/unreachable"
          operator: "Exists"
          effect: "NoExecute"
          tolerationSeconds: 86400
      initContainers:
        # --- Init containers for cloning repo and checking data (same as student) ---
        - name: clone-repo
          image: alpine:3.16
          imagePullPolicy: IfNotPresent
          command:
            - sh
            - -c
            - |
              set -ex
              echo "Installing git..."
              apk update && apk add --no-cache git
              echo "Checking for existing project repository..."
              if [ ! -d "/mnt/storage/ChordMini" ]; then
                echo "Cloning repository..."
                git clone https://gitlab.com/ptnghia-j/ChordMini.git /mnt/storage/ChordMini || {
                  echo "ERROR: Failed to clone repository"
                  exit 1
                }
              else
                echo "Repository already exists."
              fi
              exit 0
          volumeMounts:
            - name: storage
              mountPath: /mnt/storage
        - name: data-check
          image: alpine:3.16
          imagePullPolicy: IfNotPresent
          command:
            - sh
            - -c
            - |
              set -ex
              echo "Checking data directory structure..."

              # Locate the spectrogram and label directories
              PROJ_ROOT="/mnt/storage/ChordMini"
              echo "Project root: $PROJ_ROOT"

              # Create consolidated data directories if they don't exist
              # FMA dataset directories (using logits structure for all)
              mkdir -p "/mnt/storage/data/logits/synth/spectrograms"
              mkdir -p "/mnt/storage/data/logits/synth/labels"
              mkdir -p "/mnt/storage/data/logits/synth/logits"

              # Maestro dataset directories (using logits structure for all)
              mkdir -p "/mnt/storage/data/logits/maestro_synth/spectrograms"
              mkdir -p "/mnt/storage/data/logits/maestro_synth/labels"
              mkdir -p "/mnt/storage/data/logits/maestro_synth/logits"

              # DALI dataset directories (directly under data_root)
              mkdir -p "/mnt/storage/data/dali_synth/spectrograms"
              mkdir -p "/mnt/storage/data/dali_synth/labels"
              mkdir -p "/mnt/storage/data/dali_synth/logits"

              # Create symlinks from project directory to actual data location
              mkdir -p "$PROJ_ROOT/data/synth"
              mkdir -p "$PROJ_ROOT/data/maestro_synth"
              mkdir -p "$PROJ_ROOT/data/dali_synth"

              # Create unified symlinks for FMA dataset
              if [ ! -L "$PROJ_ROOT/data/synth/spectrograms" ]; then
                ln -sf "/mnt/storage/data/logits/synth/spectrograms" "$PROJ_ROOT/data/synth/spectrograms"
                echo "Created symlink for FMA spectrograms"
              fi
              if [ ! -L "$PROJ_ROOT/data/synth/labels" ]; then
                ln -sf "/mnt/storage/data/logits/synth/labels" "$PROJ_ROOT/data/synth/labels"
                echo "Created symlink for FMA labels"
              fi
              if [ ! -L "$PROJ_ROOT/data/synth/logits" ]; then
                ln -sf "/mnt/storage/data/logits/synth/logits" "$PROJ_ROOT/data/synth/logits"
                echo "Created symlink for FMA logits"
              fi

              # Create unified symlinks for Maestro dataset
              if [ ! -L "$PROJ_ROOT/data/maestro_synth/spectrograms" ]; then
                ln -sf "/mnt/storage/data/logits/maestro_synth/spectrograms" "$PROJ_ROOT/data/maestro_synth/spectrograms"
                echo "Created symlink for Maestro spectrograms"
              fi
              if [ ! -L "$PROJ_ROOT/data/maestro_synth/labels" ]; then
                ln -sf "/mnt/storage/data/logits/maestro_synth/labels" "$PROJ_ROOT/data/maestro_synth/labels"
                echo "Created symlink for Maestro labels"
              fi
              if [ ! -L "$PROJ_ROOT/data/maestro_synth/logits" ]; then
                ln -sf "/mnt/storage/data/logits/maestro_synth/logits" "$PROJ_ROOT/data/maestro_synth/logits"
                echo "Created symlink for Maestro logits"
              fi

              # Create unified symlinks for DALI dataset
              if [ ! -L "$PROJ_ROOT/data/dali_synth/spectrograms" ]; then
                ln -sf "/mnt/storage/data/dali_synth/spectrograms" "$PROJ_ROOT/data/dali_synth/spectrograms"
                echo "Created symlink for DALI spectrograms"
              fi
              if [ ! -L "$PROJ_ROOT/data/dali_synth/labels" ]; then
                ln -sf "/mnt/storage/data/dali_synth/labels" "$PROJ_ROOT/data/dali_synth/labels"
                echo "Created symlink for DALI labels"
              fi
              if [ ! -L "$PROJ_ROOT/data/dali_synth/logits" ]; then
                ln -sf "/mnt/storage/data/dali_synth/logits" "$PROJ_ROOT/data/dali_synth/logits"
                echo "Created symlink for DALI logits"
              fi

              # Check for spectrogram files in all datasets
              echo "---- Checking for FMA spectrogram files ----"
              find "/mnt/storage/data/logits/synth/spectrograms" -name "*.npy" | head -5
              echo "---- Checking for Maestro spectrogram files ----"
              find "/mnt/storage/data/logits/maestro_synth/spectrograms" -name "*.npy" | head -5
              echo "---- Checking for DALI spectrogram files ----"
              find "/mnt/storage/data/dali_synth/spectrograms" -name "*.npy" | head -5

              # Check for label files in all datasets
              echo "---- Checking for FMA label files ----"
              find "/mnt/storage/data/logits/synth/labels" -name "*.lab" | head -5
              echo "---- Checking for Maestro label files ----"
              find "/mnt/storage/data/logits/maestro_synth/labels" -name "*.lab" | head -5
              echo "---- Checking for DALI label files ----"
              find "/mnt/storage/data/dali_synth/labels" -name "*.lab" | head -5

              # Verify there are some relevant files for at least one dataset
              echo "Checking if there are data files to process..."
              FMA_SPEC_COUNT=$(find "/mnt/storage/data/logits/synth/spectrograms" -type f -name "*.npy" | wc -l)
              FMA_LABEL_COUNT=$(find "/mnt/storage/data/logits/synth/labels" -type f -name "*.lab" | wc -l)
              MAESTRO_SPEC_COUNT=$(find "/mnt/storage/data/logits/maestro_synth/spectrograms" -type f -name "*.npy" | wc -l)
              MAESTRO_LABEL_COUNT=$(find "/mnt/storage/data/logits/maestro_synth/labels" -type f -name "*.lab" | wc -l)
              DALI_SPEC_COUNT=$(find "/mnt/storage/data/dali_synth/spectrograms" -type f -name "*.npy" | wc -l)
              DALI_LABEL_COUNT=$(find "/mnt/storage/data/dali_synth/labels" -type f -name "*.lab" | wc -l)
              echo "Found $FMA_SPEC_COUNT FMA spectrogram files and $FMA_LABEL_COUNT FMA label files"
              echo "Found $MAESTRO_SPEC_COUNT Maestro spectrogram files and $MAESTRO_LABEL_COUNT Maestro label files"
              echo "Found $DALI_SPEC_COUNT DALI spectrogram files and $DALI_LABEL_COUNT DALI label files"

              if [ "$FMA_SPEC_COUNT" -eq 0 ] && [ "$MAESTRO_SPEC_COUNT" -eq 0 ] && [ "$DALI_SPEC_COUNT" -eq 0 ] || [ "$FMA_LABEL_COUNT" -eq 0 ] && [ "$MAESTRO_LABEL_COUNT" -eq 0 ] && [ "$DALI_LABEL_COUNT" -eq 0 ]; then
                echo "WARNING: No data files found in any dataset. Training may fail."
              else
                echo "Data files found. Proceeding with training."
              fi
          volumeMounts:
            - name: storage
              mountPath: /mnt/storage
      containers:
        - name: btc-trainer # Changed container name
          image: pytorch/pytorch:1.9.0-cuda11.1-cudnn8-runtime
          workingDir: /mnt/storage/ChordMini
          imagePullPolicy: IfNotPresent
          env:
            # --- Environment variables for training control ---
            - name: PYTHONPATH
              value: /mnt/storage/ChordMini
            # Learning rate scheduling options (can override btc_config.yaml)
            - name: LR_SCHEDULE
              value: "validation" # Example: Use cosine schedule
            # Learning rate parameters (can override btc_config.yaml)
            - name: LEARNING_RATE
              value: "0.0001" # Base learning rate for BTC
            - name: MIN_LEARNING_RATE
              value: "0.000001"    # Min learning rate for BTC (default 0)
            # Knowledge distillation settings (optional for BTC)
            - name: USE_KD_LOSS
              value: "true" # Set to "true" to enable KD loss if teacher logits are available
            - name: KD_ALPHA
              value: "0.2"   # Weight for KD loss (0-1)
            - name: TEMPERATURE
              value: "3.0"   # Temperature for softening distributions
            # Focal loss settings (optional for BTC)
            - name: USE_FOCAL_LOSS
              value: "true" # Set to "true" to enable focal loss
            - name: FOCAL_GAMMA
              value: "2.0"   # Focusing parameter for focal loss
            - name: FOCAL_ALPHA
              value: "0.25"  # Class weight parameter for focal loss
            # Warmup settings (optional for BTC)
            - name: USE_WARMUP
              value: "true"  # Set to "true" to enable learning rate warm-up
            - name: WARMUP_START_LR
              value: "0.0001" # Starting learning rate for warm-up
            - name: WARMUP_END_LR
              value: "0.0002"  # Final learning rate after warm-up (matches base LR)
            - name: WARMUP_EPOCHS
              value: "5"      # Number of epochs for warm-up
            # Dropout setting (can override btc_config.yaml)
            - name: DROPOUT
              value: "0.2"     # Set dropout probability (0-1)
            # Single data root for all data types
            - name: DATA_ROOT
              value: "/mnt/storage/data"
            # Add teacher checkpoint path for normalization
            - name: TEACHER_CHECKPOINT
              value: "/mnt/storage/BTC/test/btc_model_large_voca.pt"
            # GPU memory utilization optimization
            - name: GPU_MEMORY_FRACTION
              value: "0.95"  # Use 95% of available GPU memory
            # Enable distributed training
            - name: DISTRIBUTED
              value: "true"
            - name: DISTRIBUTED_BACKEND
              value: "nccl"
            # Use all available GPUs
            - name: CUDA_VISIBLE_DEVICES
              value: "0,1"
            # Dataset GPU optimization flags
            - name: BATCH_GPU_CACHE
              value: "true"  # Enable GPU batch caching
            - name: PREFETCH_FACTOR
              value: "6"     # Prefetch batches for better throughput
            # Add dataset type environment variable with combined option
            - name: DATASET_TYPE
              value: "combined"  # Options: "fma", "maestro", "dali_synth", "combined", "fma+maestro", "fma+dali_synth", or "maestro+dali_synth"
              # IMPORTANT: We're intentionally NOT using "labeled" or "labeled_synth" to avoid LabeledDataset_augmented
              # The SynthDataset class has been modified to explicitly skip LabeledDataset_augmented directories
              # The train_btc.py script has been modified to filter out 'labeled' and 'labeled_synth' from dataset types
            # Add checkpoint loading control option
            - name: LOAD_CHECKPOINT
              value: "never"  # Options: "auto" (load if exists), "never", "required"
            - name: SUB_DIR
              value: "btc-kd-combined" # Subdirectory for BTC checkpoints
            # Add reset epoch control option
            - name: RESET_EPOCH
              value: "true"  # Whether to reset epoch counter when loading a checkpoint
            # Add reset scheduler control option
            - name: RESET_SCHEDULER
              value: "true"  # Whether to reset LR scheduler when resetting epochs
            # Dataset initialization controls
            - name: LAZY_INIT
              value: "false" # Controls dataset lazy initialization
            - name: SMALL_DATASET
              value: "0.02"   # Set percentage of dataset to use (0.1 = 10%, null = full)
            - name: METADATA_CACHE
              value: "false" # Use metadata-only caching
          command:
            - sh
            - -c
            - |
              # Set environment variables for distributed training
              export MASTER_ADDR="127.0.0.1"
              export MASTER_PORT="29501" # Use different port from student training if run concurrently
              export WORLD_SIZE=2
              export RANK=0
              export LOCAL_RANK=0
              set -ex
              echo "=== BTC model training container started ==="

              # Install necessary system packages
              apt-get update && apt-get install -y libsndfile1 || { echo "ERROR: Failed to install system dependencies"; exit 1; }

              # Create a minimal requirements.txt if missing
              if [ ! -f requirements.txt ]; then
                echo "Creating minimal requirements.txt"
                echo "numpy==1.22.0" > requirements.txt
                echo "librosa>=0.8.0" >> requirements.txt
                echo "torch>=1.9.0" >> requirements.txt
                echo "tqdm>=4.62.0" >> requirements.txt
                echo "scikit-learn>=1.0.0" >> requirements.txt
                echo "pyyaml>=6.0.0" >> requirements.txt
              fi

              pip install --no-cache-dir -r requirements.txt || { echo "WARNING: Some requirements may have failed"; }
              pip install --no-cache-dir matplotlib tqdm pyyaml librosa scikit-learn || { echo "ERROR: Failed to install additional Python packages"; exit 1; }

              # Verify the data paths from the config (btc_config.yaml)
              echo "Checking data paths from config..."
              # btc_config doesn't have a paths section, rely on script logic

              # Double check that the symlinks are still valid (same as student)
              if [ ! -L "./data/synth/spectrograms" ]; then ln -sf "/mnt/storage/data/logits/synth/spectrograms" "./data/synth/spectrograms"; fi
              if [ ! -L "./data/synth/labels" ]; then ln -sf "/mnt/storage/data/logits/synth/labels" "./data/synth/labels"; fi
              if [ ! -L "./data/synth/logits" ]; then ln -sf "/mnt/storage/data/logits/synth/logits" "./data/synth/logits"; fi
              if [ ! -L "./data/maestro_synth/spectrograms" ]; then ln -sf "/mnt/storage/data/logits/maestro_synth/spectrograms" "./data/maestro_synth/spectrograms"; fi
              if [ ! -L "./data/maestro_synth/labels" ]; then ln -sf "/mnt/storage/data/logits/maestro_synth/labels" "./data/maestro_synth/labels"; fi
              if [ ! -L "./data/maestro_synth/logits" ]; then ln -sf "/mnt/storage/data/logits/maestro_synth/logits" "./data/maestro_synth/logits"; fi

              # Verify data exists for selected dataset (same logic as student)
              DATASET_ARG="--dataset_type ${DATASET_TYPE}"
              echo "Using dataset type: ${DATASET_TYPE}"
              # (Verification logic for FMA/Maestro/Combined counts omitted for brevity, same as student)

              # --- Argument construction for train_btc.py ---
              # Get LR schedule option from environment
              LR_SCHEDULE_ARG=""
              if [ "${LR_SCHEDULE}" != "" ] && [ "${LR_SCHEDULE}" != "null" ] && [ "${LR_SCHEDULE}" != "validation" ]; then
                echo "Using ${LR_SCHEDULE} learning rate schedule"
                LR_SCHEDULE_ARG="--lr_schedule ${LR_SCHEDULE}"
              else
                echo "Using default validation-based learning rate adjustment"
              fi

              # Create separate warmup argument variable
              WARMUP_ARG=""
              if [ "${USE_WARMUP}" = "true" ]; then
                echo "Using warm-up learning rate schedule for ${WARMUP_EPOCHS} epochs"
                WARMUP_ARG="--use_warmup --warmup_epochs ${WARMUP_EPOCHS}"
                if [ "${WARMUP_START_LR}" != "" ]; then WARMUP_ARG="${WARMUP_ARG} --warmup_start_lr ${WARMUP_START_LR}"; fi
                if [ "${WARMUP_END_LR}" != "" ]; then WARMUP_ARG="${WARMUP_ARG} --warmup_end_lr ${WARMUP_END_LR}"; fi
              fi

              # Get learning rate arguments
              LR_ARGS=""
              if [ "${LEARNING_RATE}" != "" ]; then LR_ARGS="${LR_ARGS} --learning_rate ${LEARNING_RATE}"; fi
              if [ "${MIN_LEARNING_RATE}" != "" ]; then LR_ARGS="${LR_ARGS} --min_learning_rate ${MIN_LEARNING_RATE}"; fi

              # Get focal loss arguments
              FOCAL_LOSS_ARG=""
              if [ "${USE_FOCAL_LOSS}" = "true" ]; then
                echo "Using focal loss with gamma=${FOCAL_GAMMA}"
                FOCAL_LOSS_ARG="--use_focal_loss --focal_gamma ${FOCAL_GAMMA}"
                if [ "${FOCAL_ALPHA}" != "" ]; then FOCAL_LOSS_ARG="${FOCAL_LOSS_ARG} --focal_alpha ${FOCAL_ALPHA}"; fi
              fi

              # Get knowledge distillation arguments
              KD_ARGS="" # KD_LOSS_ARG and KD_ARGS combined
              if [ "${USE_KD_LOSS}" = "true" ]; then
                echo "Using knowledge distillation with alpha=${KD_ALPHA} and temperature=${TEMPERATURE}"
                KD_ARGS="--use_kd_loss --kd_alpha ${KD_ALPHA} --temperature ${TEMPERATURE}"
                # Define the logits paths based on dataset type
                if [ "${DATASET_TYPE}" = "maestro" ]; then
                  KD_ARGS="${KD_ARGS} --logits_dir ${DATA_ROOT}/logits/maestro_synth/logits"
                  echo "Using Maestro logits from: ${DATA_ROOT}/logits/maestro_synth/logits"
                elif [ "${DATASET_TYPE}" = "dali_synth" ]; then
                  # For DALI dataset
                  KD_ARGS="${KD_ARGS} --logits_dir ${DATA_ROOT}/dali_synth/logits"
                  echo "Using DALI logits from: ${DATA_ROOT}/dali_synth/logits"
                elif [ "${DATASET_TYPE}" = "combined" ] || [ "${DATASET_TYPE}" = "fma+maestro" ] || [ "${DATASET_TYPE}" = "fma+dali_synth" ] || [ "${DATASET_TYPE}" = "maestro+dali_synth" ]; then
                  # For combined datasets, specify the primary FMA logits dir
                  # Assuming train_btc.py handles finding all logits like train_student.py
                  KD_ARGS="${KD_ARGS} --logits_dir ${DATA_ROOT}/logits/synth/logits" # Primary FMA path
                  echo "Using ${DATASET_TYPE} dataset - train_btc.py will find logits in standard locations"
                else # FMA
                  KD_ARGS="${KD_ARGS} --logits_dir ${DATA_ROOT}/logits/synth/logits"
                  echo "Using FMA logits from: ${DATA_ROOT}/logits/synth/logits"
                fi
              fi

              # Handle caching options
              CACHE_ARGS=""
              if [ "${METADATA_CACHE}" = "true" ]; then CACHE_ARGS="--metadata_cache"; fi
              if [ "${LAZY_INIT}" = "true" ]; then CACHE_ARGS="${CACHE_ARGS} --lazy_init"; fi
              if [ "${CACHE_FRACTION}" != "" ] && [ "${CACHE_FRACTION}" != "0.1" ]; then CACHE_ARGS="${CACHE_ARGS} --cache_fraction ${CACHE_FRACTION}"; fi
              if [ "${DISABLE_CACHE}" = "true" ]; then CACHE_ARGS="${CACHE_ARGS} --disable_cache"; fi

              # GPU acceleration options
              GPU_ARGS=""
              if [ "${GPU_MEMORY_FRACTION}" != "" ]; then GPU_ARGS="--gpu_memory_fraction ${GPU_MEMORY_FRACTION}"; fi
              if [ "${BATCH_GPU_CACHE}" = "true" ]; then GPU_ARGS="${GPU_ARGS} --batch_gpu_cache"; fi
              if [ "${PREFETCH_FACTOR}" != "" ]; then GPU_ARGS="${GPU_ARGS} --prefetch_factor ${PREFETCH_FACTOR}"; fi

              # Small dataset argument
              SMALL_DATASET_ARG=""
              if [ "${SMALL_DATASET}" != "" ] && [ "${SMALL_DATASET}" != "null" ]; then
                echo "Using only ${SMALL_DATASET} fraction of dataset"
                SMALL_DATASET_ARG="--small_dataset ${SMALL_DATASET}"
              fi

              # Get dropout from environment
              DROPOUT_ARG=""
              if [ "${DROPOUT}" != "" ]; then DROPOUT_ARG="--dropout ${DROPOUT}"; fi

              # Get teacher checkpoint argument
              TEACHER_CHECKPOINT_ARG=""
              if [ "${TEACHER_CHECKPOINT}" != "" ]; then
                echo "Using teacher checkpoint for normalization: ${TEACHER_CHECKPOINT}"
                TEACHER_CHECKPOINT_ARG="--teacher_checkpoint ${TEACHER_CHECKPOINT}"
              else
                echo "WARNING: No teacher checkpoint specified for normalization, using defaults."
              fi

              # Define checkpoint directory for BTC
              CHECKPOINTS_DIR="/mnt/storage/checkpoints/btc" # Changed directory

              # Create save directory
              SAVE_DIR="${CHECKPOINTS_DIR}"
              if [ "${SUB_DIR}" != "" ]; then
                SAVE_DIR="${CHECKPOINTS_DIR}/${SUB_DIR}"
              fi
              mkdir -p "${SAVE_DIR}"
              echo "Using checkpoint directory: ${SAVE_DIR}"

              # Create local checkpoint directory for backward compatibility
              LOCAL_CHECKPOINTS_DIR="/mnt/storage/ChordMini/checkpoints/btc"
              if [ "${LOCAL_CHECKPOINTS_DIR}" != "${SAVE_DIR}" ]; then
                mkdir -p "${LOCAL_CHECKPOINTS_DIR}"
                echo "Also created local checkpoint directory for backward compatibility: ${LOCAL_CHECKPOINTS_DIR}"
              fi

              # Handle checkpoint loading option
              CHECKPOINT_ARG=""
              BEST_CHECKPOINT_PATH="${SAVE_DIR}/btc_model_best.pth" # Changed filename
              if [ "${LOAD_CHECKPOINT}" = "auto" ]; then
                if [ -f "${BEST_CHECKPOINT_PATH}" ]; then
                  echo "Found existing checkpoint at ${BEST_CHECKPOINT_PATH}"
                  RESET_ARG=""
                  if [ "${RESET_EPOCH}" = "true" ]; then
                    RESET_ARG="--reset_epoch"
                    if [ "${RESET_SCHEDULER}" = "true" ]; then RESET_ARG="${RESET_ARG} --reset_scheduler"; fi
                  fi
                  CHECKPOINT_ARG="--load_checkpoint ${BEST_CHECKPOINT_PATH} ${RESET_ARG}"
                else
                  echo "No existing checkpoint found at ${BEST_CHECKPOINT_PATH}, starting fresh"
                fi
              elif [ "${LOAD_CHECKPOINT}" = "never" ]; then
                echo "Starting fresh training (ignoring any existing checkpoints)"
              elif [ "${LOAD_CHECKPOINT}" = "required" ]; then
                if [ -f "${BEST_CHECKPOINT_PATH}" ]; then
                  echo "Found required checkpoint at ${BEST_CHECKPOINT_PATH}"
                  RESET_ARG=""
                  if [ "${RESET_EPOCH}" = "true" ]; then
                    RESET_ARG="--reset_epoch"
                    if [ "${RESET_SCHEDULER}" = "true" ]; then RESET_ARG="${RESET_ARG} --reset_scheduler"; fi
                  fi
                  CHECKPOINT_ARG="--load_checkpoint ${BEST_CHECKPOINT_PATH} ${RESET_ARG}"
                else
                  echo "ERROR: Required checkpoint not found at ${BEST_CHECKPOINT_PATH}"
                  exit 1
                fi
              fi

              # Simplify data directory paths passed to the python script (same as student)
              if [ "${DATASET_TYPE}" = "maestro" ]; then
                SPEC_DIR_ARG="--spec_dir ${DATA_ROOT}/logits/maestro_synth/spectrograms"
                LABEL_DIR_ARG="--label_dir ${DATA_ROOT}/logits/maestro_synth/labels"
              elif [ "${DATASET_TYPE}" = "dali_synth" ]; then
                SPEC_DIR_ARG="--spec_dir ${DATA_ROOT}/dali_synth/spectrograms"
                LABEL_DIR_ARG="--label_dir ${DATA_ROOT}/dali_synth/labels"
              elif [ "${DATASET_TYPE}" = "combined" ] || [ "${DATASET_TYPE}" = "fma+maestro" ] || [ "${DATASET_TYPE}" = "fma+dali_synth" ] || [ "${DATASET_TYPE}" = "maestro+dali_synth" ]; then
                # For combined datasets, specify the primary FMA path
                # train_btc.py will handle finding all datasets based on the dataset_type
                SPEC_DIR_ARG="--spec_dir ${DATA_ROOT}/logits/synth/spectrograms" # Primary FMA path
                LABEL_DIR_ARG="--label_dir ${DATA_ROOT}/logits/synth/labels"   # Primary FMA path
              else # FMA
                SPEC_DIR_ARG="--spec_dir ${DATA_ROOT}/logits/synth/spectrograms"
                LABEL_DIR_ARG="--label_dir ${DATA_ROOT}/logits/synth/labels"
              fi

              # Run the BTC training script
              echo "Starting BTC training with GPU optimization and all options..."
              python -m torch.distributed.launch \
                --nproc_per_node=2 \
                --master_addr="127.0.0.1" \
                --master_port="29501" \
                train_btc.py \
                --distributed \
                --distributed_backend="nccl" \
                --config ./config/btc_config.yaml \
                --save_dir ${SAVE_DIR} \
                $DATASET_ARG $SPEC_DIR_ARG $LABEL_DIR_ARG \
                ${LR_SCHEDULE_ARG} ${WARMUP_ARG} ${FOCAL_LOSS_ARG} ${KD_ARGS} \
                ${CACHE_ARGS} ${GPU_ARGS} ${DROPOUT_ARG} ${LR_ARGS} \
                ${CHECKPOINT_ARG} ${SMALL_DATASET_ARG} ${TEACHER_CHECKPOINT_ARG} # Add teacher checkpoint arg

              echo "=== BTC model training complete ==="
          resources:
            requests:
              cpu: "4"
              #memory: "196Gi"
              #memory: "128Gi"
              memory: "64Gi"
              nvidia.com/gpu: "2"
            limits:
              cpu: "18"
              #memory: "260Gi"
              #memory: "160Gi"
              memory: "128Gi"
              nvidia.com/gpu: "2"
          volumeMounts:
            - name: storage
              mountPath: /mnt/storage
      volumes:
        - name: storage
          persistentVolumeClaim:
            claimName: temporary-storage-1
