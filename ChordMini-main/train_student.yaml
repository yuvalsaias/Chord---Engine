apiVersion: batch/v1
kind: Job
metadata:
  name: chordnet-fma
  namespace: csuf-titans
  labels:
    app: chord-student-training
spec:
  backoffLimit: 2
  template:
    metadata:
      labels:
        app: chord-student-training
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: nvidia.com/gpu.product
                operator: In
                values:
                # - NVIDIA-GeForce-RTX-3090
                - NVIDIA-A10
      restartPolicy: Never
      tolerations:
        - key: "node.kubernetes.io/not-ready"
          operator: "Exists"
          effect: "NoExecute"
          tolerationSeconds: 86400
        - key: "node.kubernetes.io/unreachable"
          operator: "Exists"
          effect: "NoExecute"
          tolerationSeconds: 86400
      initContainers:
        - name: clone-repo
          image: alpine:3.16
          imagePullPolicy: IfNotPresent
          command:
            - sh
            - -c
            - |
              set -ex
              echo "Installing git..."
              apk update && apk add --no-cache git
              echo "Checking for existing project repository..."
              if [ ! -d "/mnt/storage/ChordMini" ]; then
                echo "Cloning repository..."
                git clone https://gitlab.com/ptnghia-j/ChordMini.git /mnt/storage/ChordMini || {
                  echo "ERROR: Failed to clone repository"
                  exit 1
                }
              else
                echo "Repository already exists."
              fi
              exit 0
          volumeMounts:
            - name: storage
              mountPath: /mnt/storage
        - name: data-check
          image: alpine:3.16
          imagePullPolicy: IfNotPresent
          command:
            - sh
            - -c
            - |
              set -ex
              echo "Checking data directory structure..."

              # Locate the spectrogram and label directories
              PROJ_ROOT="/mnt/storage/ChordMini"
              echo "Project root: $PROJ_ROOT"

              # Create the data directories if they don't exist for both dataset types
              # FMA dataset directories
              mkdir -p "/mnt/storage/data/logits/synth/spectrograms"
              mkdir -p "/mnt/storage/data/logits/synth/labels"
              mkdir -p "/mnt/storage/data/logits/synth/logits"

              # Maestro dataset directories
              mkdir -p "/mnt/storage/data/maestro_synth/spectrograms"
              mkdir -p "/mnt/storage/data/maestro_synth/labels"
              mkdir -p "/mnt/storage/data/maestro_synth/logits"

              # Create symlinks from project directory to actual data location if needed
              mkdir -p "$PROJ_ROOT/data/synth"
              mkdir -p "$PROJ_ROOT/data/maestro_synth"

              # Create symlinks for FMA dataset
              if [ -L "$PROJ_ROOT/data/synth/spectrograms" ]; then
                echo "Symlink for FMA spectrograms already exists"
              else
                ln -sf "/mnt/storage/data/logits/synth/spectrograms" "$PROJ_ROOT/data/synth/spectrograms"
                echo "Created symlink for FMA spectrograms"
              fi

              if [ -L "$PROJ_ROOT/data/synth/labels" ]; then
                echo "Symlink for FMA labels already exists"
              else
                ln -sf "/mnt/storage/data/logits/synth/labels" "$PROJ_ROOT/data/synth/labels"
                echo "Created symlink for FMA labels"
              fi

              if [ -L "$PROJ_ROOT/data/synth/logits" ]; then
                echo "Symlink for FMA logits already exists"
              else
                ln -sf "/mnt/storage/data/logits/synth/logits" "$PROJ_ROOT/data/synth/logits"
                echo "Created symlink for FMA logits"
              fi

              # Create symlinks for Maestro dataset
              if [ -L "$PROJ_ROOT/data/maestro_synth/spectrograms" ]; then
                echo "Symlink for Maestro spectrograms already exists"
              else
                ln -sf "/mnt/storage/data/maestro_synth/spectrograms" "$PROJ_ROOT/data/maestro_synth/spectrograms"
                echo "Created symlink for Maestro spectrograms"
              fi

              if [ -L "$PROJ_ROOT/data/maestro_synth/labels" ]; then
                echo "Symlink for Maestro labels already exists"
              else
                ln -sf "/mnt/storage/data/maestro_synth/labels" "$PROJ_ROOT/data/maestro_synth/labels"
                echo "Created symlink for Maestro labels"
              fi

              if [ -L "$PROJ_ROOT/data/maestro_synth/logits" ]; then
                echo "Symlink for Maestro logits already exists"
              else
                ln -sf "/mnt/storage/data/maestro_synth/logits" "$PROJ_ROOT/data/maestro_synth/logits"
                echo "Created symlink for Maestro logits"
              fi

              # Check for spectrogram files in both datasets
              echo "---- Checking for FMA spectrogram files ----"
              find "/mnt/storage/data/logits/synth/spectrograms" -name "*.npy" | head -5

              echo "---- Checking for Maestro spectrogram files ----"
              find "/mnt/storage/data/maestro_synth" -name "*.npy" | head -5

              # Check for label files in both datasets
              echo "---- Checking for FMA label files ----"
              find "/mnt/storage/data/logits/synth/labels" -name "*.lab" | head -5

              echo "---- Checking for Maestro label files ----"
              find "/mnt/storage/data/maestro_synth" -name "*.lab" | head -5

              # Verify there are some relevant files for at least one dataset
              echo "Checking if there are data files to process..."
              FMA_SPEC_COUNT=$(find "/mnt/storage/data/logits/synth/spectrograms" -type f -name "*.npy" | wc -l)
              FMA_LABEL_COUNT=$(find "/mnt/storage/data/logits/synth/labels" -type f -name "*.lab" | wc -l)

              MAESTRO_SPEC_COUNT=$(find "/mnt/storage/data/maestro_synth" -type f -name "*.npy" | wc -l)
              MAESTRO_LABEL_COUNT=$(find "/mnt/storage/data/maestro_synth" -type f -name "*.lab" | wc -l)

              echo "Found $FMA_SPEC_COUNT FMA spectrogram files and $FMA_LABEL_COUNT FMA label files"
              echo "Found $MAESTRO_SPEC_COUNT Maestro spectrogram files and $MAESTRO_LABEL_COUNT Maestro label files"

              if [ "$FMA_SPEC_COUNT" -eq 0 ] && [ "$MAESTRO_SPEC_COUNT" -eq 0 ] || [ "$FMA_LABEL_COUNT" -eq 0 ] && [ "$MAESTRO_LABEL_COUNT" -eq 0 ]; then
                echo "WARNING: No data files found in either dataset. Training may fail."
              else
                echo "Data files found. Proceeding with training."
              fi
          volumeMounts:
            - name: storage
              mountPath: /mnt/storage
      containers:
        # !!! It is recommended to test the model on small portion of the dataset first
        - name: student-trainer
          image: pytorch/pytorch:1.9.0-cuda11.1-cudnn8-runtime
          workingDir: /mnt/storage/ChordMini
          imagePullPolicy: IfNotPresent
          env:
            # These environment variables override defaults in student_config.yaml
            - name: PYTHONPATH
              value: /mnt/storage/ChordMini
            # Model scale - overrides student_config.yaml
            - name: MODEL_SCALE
              value: "1.0"  # Default scale: 1.0 = base model, 0.5 = half-scale, 2.0 = double
            # Learning rate scheduling options - overrides student_config.yaml settings
            - name: LR_SCHEDULE
              value: "validation"  # Changed from empty string to 'validation'
            # Additional learning rate parameters - new additions
            - name: LEARNING_RATE
              value: "0.0001"  # Base learning rate
            - name: MIN_LEARNING_RATE
              value: "0.000001"  # Minimum learning rate for schedulers
            # Knowledge distillation settings
            - name: USE_KD_LOSS
              value: "false"  # Set to "true" to enable KD loss
            - name: KD_ALPHA
              value: "0.2"  # Weight for KD loss (0-1)
            - name: TEMPERATURE
              value: "4.0"  # Temperature for softening distributions
            # Focal loss settings
            - name: USE_FOCAL_LOSS
              value: "true"  # Set to "true" to enable focal loss
            - name: FOCAL_GAMMA
              value: "2.0"   # Focusing parameter for focal loss
            - name: FOCAL_ALPHA
              value: "0.25"  # Class weight parameter for focal loss
            # Warmup settings
            - name: USE_WARMUP
              value: "true"  # Set to "true" to enable learning rate warm-up
            - name: WARMUP_START_LR
              value: "0.0001"  # Starting learning rate for warm-up
            - name: WARMUP_END_LR
              value: "0.0003"  # Final learning rate after warm-up
            - name: WARMUP_EPOCHS
              value: "10"  # Number of epochs for warm-up
            # Dropout setting
            - name: DROPOUT
              value: "0.3"  # Set dropout probability (0-1)
            # Single data root for all data types
            - name: DATA_ROOT
              value: "/mnt/storage/data"
            # Add teacher checkpoint path for normalization
            - name: TEACHER_CHECKPOINT
              value: "/mnt/storage/BTC/test/btc_model_large_voca.pt" # Path to the teacher model for norm stats
            # GPU memory utilization optimization
            - name: GPU_MEMORY_FRACTION
              value: "0.95"  # Use 95% of available GPU memory
            - name: CUDA_VISIBLE_DEVICES
              value: "0"
            # Dataset GPU optimization flags
            - name: BATCH_GPU_CACHE
              value: "true"  # Enable GPU batch caching
            - name: PREFETCH_FACTOR
              value: "6"     # ! NO EFFECT if num_workers=0
            # Add dataset type environment variable
            - name: DATASET_TYPE
              value: "fma"  # Use "maestro" for Maestro dataset or "fma" for FMA dataset
            # Add checkpoint loading control option
            - name: LOAD_CHECKPOINT
              value: "never"  # Options: "auto" (load if exists), "never" (always start fresh), "required" (must exist)
            - name: SUB_DIR
              value: "chordnet-fma"  # This will be used to append to the save directory for checkpoints, can be empty or set to a specific subdirectory if needed
            # Add reset epoch control option
            - name: RESET_EPOCH
              value: "true"  # Whether to reset epoch counter when loading a checkpoint (true/false)
            # Add reset scheduler control option
            - name: RESET_SCHEDULER
              value: "true"  # Whether to reset LR scheduler when resetting epochs (true/false)
            # Dataset initialization controls
            - name: LAZY_INIT
              value: "false"  # Controls dataset lazy initialization (true/false)
            - name: SMALL_DATASET
              value: ""  # Set percentage of dataset to use (0.1 = 10%, null = full dataset)
            - name: METADATA_CACHE
              value: "false"  # Use metadata-only caching to reduce memory usage
          command:
            - sh
            - -c
            - |
              set -ex
              echo "=== Student model training container started ==="

              # Install necessary system packages
              apt-get update && apt-get install -y libsndfile1 || { echo "ERROR: Failed to install system dependencies"; exit 1; }

              # Create a minimal requirements.txt if missing
              if [ ! -f requirements.txt ]; then
                echo "Creating minimal requirements.txt"
                echo "numpy==1.22.0" > requirements.txt
                echo "librosa>=0.8.0" >> requirements.txt
                echo "torch>=1.9.0" >> requirements.txt
                echo "tqdm>=4.62.0" >> requirements.txt
                echo "scikit-learn>=1.0.0" >> requirements.txt
                echo "pyyaml>=6.0.0" >> requirements.txt  # Added for config loading
              fi

              pip install --no-cache-dir -r requirements.txt || { echo "WARNING: Some requirements may have failed"; }
              pip install --no-cache-dir matplotlib tqdm pyyaml librosa scikit-learn || { echo "ERROR: Failed to install additional Python packages"; exit 1; }

              # Verify the data paths from the config
              echo "Checking data paths from config..."
              grep -A 10 "paths:" ./config/student_config.yaml

              # Double check that the symlinks are still valid
              if [ ! -L "./data/synth/spectrograms" ]; then
                echo "Recreating symlink for spectrograms"
                mkdir -p ./data/synth
                ln -sf "/mnt/storage/data/logits/synth/spectrograms" "./data/synth/spectrograms"
              fi

              if [ ! -L "./data/synth/labels" ]; then
                echo "Recreating symlink for labels"
                mkdir -p ./data/synth
                ln -sf "/mnt/storage/data/logits/synth/labels" "./data/synth/labels"
              fi

              if [ ! -L "./data/synth/logits" ]; then
                echo "Recreating symlink for logits"
                mkdir -p ./data/synth
                ln -sf "/mnt/storage/data/logits/synth/logits" "./data/synth/logits"
              fi

              # Verify that spectrograms and labels exist in the expected location
              SPEC_COUNT=$(find ./data/synth/spectrograms -type f -name "*.npy" | wc -l)
              LABEL_COUNT=$(find ./data/synth/labels -type f -name "*.lab" | wc -l)
              echo "Found $SPEC_COUNT spectrogram files and $LABEL_COUNT label files"

              # Before starting training, verify data exists for selected dataset
              if [ "${DATASET_TYPE}" = "maestro" ]; then
                SPEC_COUNT=$(find ./data/maestro_synth/spectrograms -type f -name "*.npy" | wc -l)
                LABEL_COUNT=$(find ./data/maestro_synth/labels -type f -name "*.lab" | wc -l)
                echo "Using Maestro dataset: Found $SPEC_COUNT spectrogram files and $LABEL_COUNT label files"

                # Add dataset type to training command
                DATASET_ARG="--dataset_type maestro"
              else
                SPEC_COUNT=$(find ./data/synth/spectrograms -type f -name "*.npy" | wc -l)
                LABEL_COUNT=$(find ./data/synth/labels -type f -name "*.lab" | wc -l)
                echo "Using FMA dataset: Found $SPEC_COUNT spectrogram files and $LABEL_COUNT label files"

                # Add dataset type to training command
                DATASET_ARG="--dataset_type fma"
              fi

              # Before starting training, verify data exists
              if [ "$SPEC_COUNT" -eq 0 ] || [ "$LABEL_COUNT" -eq 0 ]; then
                echo "WARNING: No data found in the expected paths"
                echo "Looking for data in alternate locations..."

                ALT_SPEC_COUNT=$(find /mnt/storage -type f -name "*.npy" | wc -l)
                ALT_LABEL_COUNT=$(find /mnt/storage -type f -name "*.lab" | wc -l)
                echo "Found $ALT_SPEC_COUNT .npy files and $ALT_LABEL_COUNT .lab files in alternate locations"

                # Show where the files were found
                if [ "$ALT_SPEC_COUNT" -gt 0 ]; then
                  echo "Sample spectrogram locations:"
                  find /mnt/storage -type f -name "*.npy" | head -5
                fi

                if [ "$ALT_LABEL_COUNT" -gt 0 ]; then
                  echo "Sample label locations:"
                  find /mnt/storage -type f -name "*.lab" | head -5
                fi
              fi

              # Get LR schedule option from environment
              LR_SCHEDULE_ARG=""
              if [ "${LR_SCHEDULE}" != "" ] && [ "${LR_SCHEDULE}" != "null" ]; then
                echo "Using ${LR_SCHEDULE} learning rate schedule"
                LR_SCHEDULE_ARG="--lr_schedule ${LR_SCHEDULE}"
              else
                echo "Using default validation-based learning rate adjustment"
              fi

              # Create separate warmup argument variable that's always set when warmup is enabled
              WARMUP_ARG=""
              if [ "${USE_WARMUP}" = "true" ]; then
                echo "Using warm-up learning rate schedule for ${WARMUP_EPOCHS} epochs"
                WARMUP_ARG="--use_warmup --warmup_epochs ${WARMUP_EPOCHS}"

                # Add the other warmup parameters to ensure they're passed to the script
                if [ "${WARMUP_START_LR}" != "" ]; then
                  WARMUP_ARG="${WARMUP_ARG} --warmup_start_lr ${WARMUP_START_LR}"
                  echo "Using warmup start learning rate: ${WARMUP_START_LR}"
                fi

                if [ "${WARMUP_END_LR}" != "" ]; then
                  WARMUP_ARG="${WARMUP_ARG} --warmup_end_lr ${WARMUP_END_LR}"
                  echo "Using warmup end learning rate: ${WARMUP_END_LR}"
                fi
              fi

              # Get learning rate arguments
              LR_ARGS=""
              if [ "${LEARNING_RATE}" != "" ]; then
                echo "Using base learning rate: ${LEARNING_RATE}"
                LR_ARGS="${LR_ARGS} --learning_rate ${LEARNING_RATE}"
              fi

              if [ "${MIN_LEARNING_RATE}" != "" ]; then
                echo "Using minimum learning rate: ${MIN_LEARNING_RATE}"
                LR_ARGS="${LR_ARGS} --min_learning_rate ${MIN_LEARNING_RATE}"
              fi

              if [ "${WARMUP_START_LR}" != "" ]; then
                echo "Using warmup start learning rate: ${WARMUP_START_LR}"
                LR_ARGS="${LR_ARGS} --warmup_start_lr ${WARMUP_START_LR}"
              fi

              if [ "${WARMUP_END_LR}" != "" ]; then
                echo "Using warmup end learning rate: ${WARMUP_END_LR}"
                LR_ARGS="${LR_ARGS} --warmup_end_lr ${WARMUP_END_LR}"
              fi

              # Get focal loss arguments
              FOCAL_LOSS_ARG=""
              if [ "${USE_FOCAL_LOSS}" = "true" ]; then
                echo "Using focal loss with gamma=${FOCAL_GAMMA}"
                FOCAL_LOSS_ARG="--use_focal_loss --focal_gamma ${FOCAL_GAMMA}"
                # Add focal_alpha if specified
                if [ "${FOCAL_ALPHA}" != "" ]; then
                  FOCAL_LOSS_ARG="${FOCAL_LOSS_ARG} --focal_alpha ${FOCAL_ALPHA}"
                  echo "Using focal loss alpha=${FOCAL_ALPHA}"
                fi
              else
                echo "Using standard cross-entropy loss (focal loss disabled)"
              fi

              # Get knowledge distillation arguments
              KD_LOSS_ARG=""
              if [ "${USE_KD_LOSS}" = "true" ]; then
                echo "Using knowledge distillation with alpha=${KD_ALPHA} and temperature=${TEMPERATURE}"
                KD_LOSS_ARG="--use_kd_loss --kd_alpha ${KD_ALPHA} --temperature ${TEMPERATURE} --logits_dir ${DATA_ROOT}/logits/synth/logits"
                echo "Note: Teacher logits must be included in the batch data"
              fi

              # Get model scale from environment
              MODEL_SCALE_ARG=""
              if [ "${MODEL_SCALE}" != "" ] && [ "${MODEL_SCALE}" != "1.0" ]; then
                echo "Using model scale factor: ${MODEL_SCALE}"
                MODEL_SCALE_ARG="--model_scale ${MODEL_SCALE}"
              else
                echo "Using default model scale (1.0)"
              fi

              # Handle caching options for memory optimization
              CACHE_ARGS=""
              if [ "${METADATA_CACHE}" = "true" ]; then
                echo "Using metadata-only caching to reduce memory usage"
                CACHE_ARGS="--metadata_cache"
              else
                echo "Dataset caching disabled to reduce memory usage"
                CACHE_ARGS="--disable_cache"
              fi

              # Add cache fraction argument
              if [ "${CACHE_FRACTION}" != "" ] && [ "${CACHE_FRACTION}" != "1.0" ]; then
                echo "Using partial dataset caching: ${CACHE_FRACTION} of samples"
                CACHE_ARGS="${CACHE_ARGS} --cache_fraction ${CACHE_FRACTION}"
              fi

              # Handle data paths regardless of KD setting
              DATA_ARGS=""
              if [ -d "${DATA_ROOT}/logits/synth" ]; then
                echo "Using data from: ${DATA_ROOT}/logits/synth"

                # Define paths based on directory structure
                if [ -d "${DATA_ROOT}/logits/synth/spectrograms" ] && [ -d "${DATA_ROOT}/logits/synth/labels" ] && [ -d "${DATA_ROOT}/logits/synth/logits" ]; then
                  # Standard structure with separate directories
                  echo "Found complete directory structure with spectrograms, labels, and logits"
                  DATA_ARGS="--spec_dir ${DATA_ROOT}/logits/synth/spectrograms --label_dir ${DATA_ROOT}/logits/synth/labels"
                  echo "Using spectrograms and labels from the logits directory"
                else
                  # All files might be in the root directory
                  DATA_ARGS="--spec_dir ${DATA_ROOT}/logits/synth --label_dir ${DATA_ROOT}/logits/synth"
                  echo "Using flat directory structure"
                fi

                # Log the actual paths being used
                echo "Data path arguments: ${DATA_ARGS}"
              else
                echo "WARNING: Data directory does not exist: ${DATA_ROOT}/logits/synth"
                echo "Attempting to use default paths"
              fi

              # Handle KD paths when KD is enabled
              KD_ARGS=""
              if [ "${USE_KD_LOSS}" = "true" ]; then
                echo "Knowledge distillation enabled - using teacher logits"

                # Check if KD_DATA_ROOT exists
                if [ -d "${DATA_ROOT}/logits/synth/logits" ]; then
                  echo "Using KD logits from: ${DATA_ROOT}/logits/synth/logits"
                  KD_ARGS="--logits_dir ${DATA_ROOT}/logits/synth/logits"
                else
                  echo "WARNING: Logits directory does not exist: ${DATA_ROOT}/logits/synth/logits"
                  echo "Attempting to use default paths"
                  KD_ARGS="--logits_dir /mnt/storage/data/logits/synth/logits"

                  # Create parent directories to avoid errors later
                  mkdir -p "/mnt/storage/data/logits/synth/logits"
                fi

                # Log the actual paths being used
                echo "KD arguments: ${KD_ARGS}"
              fi

              # GPU acceleration options
              GPU_ARGS=""
              # Remove the following line as --gpu_memory_fraction is not a valid argument
              # if [ "${GPU_MEMORY_FRACTION}" != "" ]; then
              #   GPU_ARGS="--gpu_memory_fraction ${GPU_MEMORY_FRACTION}"
              # fi

              if [ "${BATCH_GPU_CACHE}" = "true" ]; then
                GPU_ARGS="${GPU_ARGS} --batch_gpu_cache"
              fi

              if [ "${PREFETCH_FACTOR}" != "" ]; then
                GPU_ARGS="${GPU_ARGS} --prefetch_factor ${PREFETCH_FACTOR}"
              fi

              # Dataset initialization control arguments
              DATASET_INIT_ARGS=""
              if [ "${LAZY_INIT}" = "true" ]; then
                echo "Using lazy initialization for dataset"
                DATASET_INIT_ARGS="--lazy_init"
              else
                echo "Disabling lazy initialization for dataset (will pre-load all metadata)"
              fi

              if [ "${SMALL_DATASET}" != "" ] && [ "${SMALL_DATASET}" != "null" ]; then
                echo "Using only ${SMALL_DATASET} fraction of dataset (${SMALL_DATASET}%)"
                DATASET_INIT_ARGS="${DATASET_INIT_ARGS} --small_dataset ${SMALL_DATASET}"
              else
                echo "Using full dataset"
              fi

              # Get dropout from environment
              DROPOUT_ARG=""
              if [ "${DROPOUT}" != "" ]; then
                echo "Using dropout value: ${DROPOUT}"
                DROPOUT_ARG="--dropout ${DROPOUT}"
              else
                echo "Using default dropout value from config"
              fi

              # ! NOTE: Define checkpoint
              CHECKPOINTS_DIR="/mnt/storage/checkpoints/student"

              # ! NOTE: save checkpoint directory defined in env
              if [ "${SUB_DIR}" != "" ]; then
                # Create directory with subdirectory if SUB_DIR is specified
                mkdir -p "${CHECKPOINTS_DIR}/${SUB_DIR}"
                echo "Using checkpoint directory: ${CHECKPOINTS_DIR}/${SUB_DIR}"
              else
                # Create just the base directory
                mkdir -p "${CHECKPOINTS_DIR}"
                echo "Using checkpoint directory: ${CHECKPOINTS_DIR}"
              fi

              # Handle checkpoint loading option
              CHECKPOINT_ARG=""
              if [ "${LOAD_CHECKPOINT}" = "auto" ]; then
                if [ -f "${CHECKPOINTS_DIR}/student_model_best.pth" ]; then
                  echo "Found existing checkpoint at ${CHECKPOINTS_DIR}/student_model_best.pth"
                  if [ "${RESET_EPOCH}" = "true" ]; then
                    RESET_ARG="--reset_epoch"
                    if [ "${RESET_SCHEDULER}" = "true" ]; then
                      echo "Will load weights but start training from epoch 1 with fresh learning rate schedule"
                      RESET_ARG="${RESET_ARG} --reset_scheduler"
                    else
                      echo "Will load weights but start training from epoch 1 (keeping scheduler state)"
                    fi
                    CHECKPOINT_ARG="--load_checkpoint ${CHECKPOINTS_DIR}/student_model_best.pth ${RESET_ARG}"
                  else
                    echo "Will load weights and continue training from saved epoch"
                    CHECKPOINT_ARG="--load_checkpoint ${CHECKPOINTS_DIR}/student_model_best.pth"
                  fi
                else
                  echo "No existing checkpoint found at ${CHECKPOINTS_DIR}/student_model_best.pth"
                  echo "Starting training from scratch"
                fi
              elif [ "${LOAD_CHECKPOINT}" = "never" ]; then
                echo "Starting fresh training (ignoring any existing checkpoints)"
              elif [ "${LOAD_CHECKPOINT}" = "required" ]; then
                if [ -f "${CHECKPOINTS_DIR}/student_model_best.pth" ]; then
                  if [ "${RESET_EPOCH}" = "true" ]; then
                    RESET_ARG="--reset_epoch"
                    if [ "${RESET_SCHEDULER}" = "true" ]; then
                      echo "Found required checkpoint at ${CHECKPOINTS_DIR}/student_model_best.pth"
                      echo "Will load weights but start training from epoch 1 with fresh learning rate schedule"
                      RESET_ARG="${RESET_ARG} --reset_scheduler"
                    else
                      echo "Found required checkpoint at ${CHECKPOINTS_DIR}/student_model_best.pth"
                      echo "Will load weights but start training from epoch 1 (keeping scheduler state)"
                    fi
                    CHECKPOINT_ARG="--load_checkpoint ${CHECKPOINTS_DIR}/student_model_best.pth ${RESET_ARG}"
                  else
                    echo "ERROR: Required checkpoint not found at ${CHECKPOINTS_DIR}/student_model_best.pth"
                    echo "Cannot proceed with training"
                    exit 1
                  fi
                fi
              fi

              # Add teacher checkpoint argument
              TEACHER_CHECKPOINT_ARG=""
              if [ -n "${TEACHER_CHECKPOINT}" ]; then
                echo "Loading normalization from teacher checkpoint: ${TEACHER_CHECKPOINT}"
                TEACHER_CHECKPOINT_ARG="--teacher_checkpoint ${TEACHER_CHECKPOINT}"
              else
                echo "WARNING: No teacher checkpoint specified for normalization. Trainer might calculate its own or use defaults."
              fi

              # Run the student training script with all options (without --best_model_dir flag)
              echo "Starting student training with GPU optimization and all options..."
              python train_student.py $DATASET_ARG --config ./config/student_config.yaml --save_dir ${CHECKPOINTS_DIR}/${SUB_DIR} \
                ${LR_SCHEDULE_ARG} ${WARMUP_ARG} ${FOCAL_LOSS_ARG} ${KD_LOSS_ARG} ${MODEL_SCALE_ARG} ${CACHE_ARGS} ${DATA_ARGS} ${KD_ARGS} \
                ${GPU_ARGS} ${BATCH_OVERRIDE} ${DROPOUT_ARG} ${LR_ARGS} ${CHECKPOINT_ARG} ${DATASET_INIT_ARGS} ${TEACHER_CHECKPOINT_ARG} # Removed reference to GPU_ARGS containing gpu_memory_fraction

              echo "=== Student model training complete ==="
          resources:
            requests:
              cpu: "4"
              memory: "24Gi"
              nvidia.com/gpu: "1"
            limits:
              cpu: "16"
              memory: "64Gi"
              nvidia.com/gpu: "1"
          volumeMounts:
            - name: storage
              mountPath: /mnt/storage
      volumes:
        - name: storage
          persistentVolumeClaim:
            claimName: temporary-storage